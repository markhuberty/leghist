%\documentclass[14pt]{extarticle}
\documentclass[11pt]{article}
\usepackage{natbib}
\usepackage[dvipdfm,colorlinks=true,urlcolor=DarkBlue,linkcolor=DarkBlue,bookmarks=false,citecolor=DarkBlue]{hyperref}

\usepackage[pdftex]{graphicx}
\usepackage{fancyhdr}
\usepackage[T1]{fontenc}
\usepackage{palatino}
\usepackage[utf8]{inputenc}
%\usepackage[super]{nth}
\usepackage{setspace}
\usepackage{placeins}
\usepackage{subfigure}
\usepackage{multirow}
\usepackage{rotating}
\usepackage{marvosym}  % Used for euro symbols with \EUR
\newcommand{\HRule}{\rule{\linewidth}{0.5mm}}
\usepackage{longtable} %% Allows the use of the longtable format produced by xl2latex.rb
\usepackage{lscape} %% Allows landscape orientation of tables
\usepackage{appendix} %% Allows customization of the appendix properties
\setcounter{tocdepth}{1} %% Restricts the table of contents to the section header level entries only

\usepackage{geometry}
\geometry{letterpaper}
\usepackage{amsmath}
\usepackage[stable]{footmisc}

%% The following settings are for the listings environment in R
\usepackage{listings}
\usepackage[svgnames]{xcolor}
\usepackage{soul}
\sethlcolor{LightGoldenrodYellow}
\lstset{backgroundcolor=\color{LightYellow}}
\lstset{framextopmargin=6pt, framexbottommargin=6pt, framerule=4pt, rulecolor=\color{White}}

\title{LegHist:\\ Automated legislative history analysis for
  R\thanks{Original version: 30 January 2012. Thanks to Adrienne Hosek for discussions on the
  comparative legislative processes for the United States Congress and
the European Parliament.}}
\author{Mark Huberty\thanks{Travers Department of Political Science,
    University of California, Berkeley. Contact:
    \url{markhuberty@berkeley.edu}.}~ and Hillary Sanders\thanks{University of
California, Berkeley.}}
\date{\today}

\graphicspath{{../figures/}}

\begin{document}
%\input{/Users/markhuberty/Documents/Technology/Documentation/Typesetting/prospectusTitle.tex}
\maketitle
%\doublespacing

\begin{abstract}
  
\end{abstract}

\section{Introduction}
\label{sec:introduction}

Analysis of legislative history plays a central role in political
science. The data often present barriers to making straightforward, or
quantifiable progress. Few legislatures provide marked-up versions of final bills
indicating when and how proposed legislation was amended, or by
whom. Particularly for very large bills with many amendments,
identifying these important sources of information on legislative
influence and process constitute a very large search problem. 

Here, we provide tools to automate the process of legislative history
analysis. The \texttt{leghist} package for R provides tools for
processing text from proposed legislation and amendments, matching
sections of the final bill to their most likely origin in either the
proposed bill or subsequent amendments, and quantifying the degree and
from of variation among both successful and unsuccessful
amendments. We show, through hand-coded trials, that the unsupervised
methods used here generate valid matches at a high degree of accuracy
in terms of both human-identified textual similarity and evidence from
the formal legislative record. Finally, we emphasize that the tools
provided here should work for other document history processes
analagous to the legislative history model we present below.

\section{How a bill becomes a law: a model}
\label{sec:how-bill-becomes}

The tools presented here implement an abstracted view of how a bill
becomes a law. This section presents this stylized model in three,
increasingly complex, steps. The biggest change imposed by the added
complexity concerns whether we should use supervised or unsupervised
methods to identify the origin of the components of the final
bill. While the most basic case permits a purely unsupervised
approach, the more complex cases benefit from some additional user
input. The tools provided in the
\texttt{leghist} package assist the analyst in making progress on both
the most basic case and each extension. 

We may raise a set of questions that we want to answer with regard to
the history of a bill as it evolves through a legislative process:
\begin{itemize}
\item Which sections of the original bill survived to the final version?
\item Which amendments were adopted, and which weren't?
\item Who originated the adopted amendments, versus those not adopted?
\item What was the content of the amendments, and does that content
  categorize in systematic ways (i.e., did amendments focus on
  particular topics within the policy domain)?
\item Were there aspects of the final bill that have no good match,
  suggesting changes made behind the scenes (i.e., in the
  reconcilliation process between the House and Senate in the United States Congress)
\end{itemize}


\subsection{The simple model: a pure legislative process}
\label{sec:simple-model:-pure}

We treat a bill as a document $D$ composed of $I$ sections $d_i$. The
legislative process begins with an initial bill $D_i$, and concludes
with a successful bill $D_f$. During the legislative process, $J$
amendments $a_j \in A_J$ are
proposed by some set of legislative actors $L$, of which $A_K, K \leq
J$ are adopted. For $D_i, D_f,$ and $A_J$, we assume that the sections
$d_{i,i}, d_{f,i}$, and $a_j$ are all at the same level of
disaggregation. The tools presented here are agnostic to that level of
disaggregation, but for the rest of this paper we assume that sections
are analagous to paragraphs. This is consistent with the structure of
amendments for the European Parliamentary bills considered in section
\ref{sec:vign-2009-europ}.

For the simple case, then, the final bill is a
combination of the proposed bill and the proposed
amendments. We can represent that process as a functional form of $D_f
= f(D_i, A_j)$. Each section $d_{f,i}$ contains one and only one best
match in $D_i, A_j$. Given that, the challenge is to identify the sections
of the original bill and the amendments that comprise the final
bill. 

We can thus define a similarity measure $S$. For simiplicity, we
assume that $S \in [0,1]$. For each final section $d_{f,i}$ we
construct a distance vector $s_i$ between all possible matches in
$D_i, A_j$. The best match is thus $arg max(d_i, a_j) s_i$. We can
therefore use nearest-neighbor matching, since for each $d_{f,i}$
there is one and only one best match.

To construct the distance metric, we first construct a vector-space
representation of each section of $D_f$, $D_i$, and $A_j$, using a
common feature set. Given that vector-space representation of the
language used in the final bill and all candidate matches, we can
construct the similarity measure using any continuously-valued
distance or similarity metric. 

\subsection{Extensions: fuzzy matches}
\label{sec:extens-fuzzy-match}

The first extension to this stylized legislative process involves a
set of fuzzy potential matches. Two possible developments will lead to
fuzzy matches. First, specific language in a bill, such as
cross-references to other bill sections, dates, or formal legal
language, may change without affecting the substantive semantic
content of the bill section and its best match. Second, the drafting
process may split apart and recombine sections prior to the final
bill, without necessarily affecting the origin of the legislative
language.\footnote{We also note that this set of changes may be
  introduced by the process of producing machine-readable text. Given
  that much of this text must be scraped by the analyst, this form of
  variation may be introduced by the scraping algorithm
  itself. Validating the algorithm for each bill represents a
  significant effort that we would like the analyst to avoid.} We would wish to identify both cases.

We handle both cases as follows. By implementing nearest-neighbor
matching with replacement, we enable matches between a single
source paragraph and multiple final paragraphs. 

This same approach also works for subtle changes in language
introduced during final drafting of the bill. For instance, consider
the final section of a bill (left) and the initial section of a bill
(right) presented as below:
\begin{quote}
  \begin{minipage}[t]{0.45\linewidth}
    to develop renewable energies to meet the commitment of the Com-
    munity to using 20\% renewable energies by 2020, as well as to
    develop other technologies contributing to the transition to a
    safe and sustainable low-carbon economy and to help meet the commitment of the Community to increase energy effciency by 20\% by
    2020;
  \end{minipage}
  \begin{minipage}[h]{0.08\linewidth}
  \end{minipage}
  \begin{minipage}[t]{0.45\linewidth}
    to develop renewable energies to meet the commitment of the Com-
    munity to using 20\% renewable energies by 2020, and to meet the
    commitment of the Community to increase energy effciency by 20\%
    by 2020;
  \end{minipage}
\end{quote}

Here, the use of nearest-neighbor matching does not require the exact
match between the final bill section and its candidate
sources. Rather, so long as the similarity measure produces a
well-ordered similarity vector, the match will return the correct value.

It is important to note one situation in which this matching process
breaks down. Introduction of the same amendment by multiple
legislative actors will produce more than one match, since the
similarity vector is no longer well-ordered. Absent additional
information, it is impossible to determine which is the ``correct''
match from this set. We will return to this issue when we discuss
supervised processes.

\subsection{Extensions: negotiated content and revisions}
\label{sec:extens-negot-cont}

Finally, some legislative processes may not require that all changes
to a bill be introduced as formal amendments. This is particularly
true where negotiated outcomes are common. Examples here include the
reconciliation process in the United States Congress, and the
co-decision process between the European Parliament and Council. In
this case, substantial changes to a bill may occur without any
matching record in either the original bill or the proposed
amendments. 

Given that the nearest-neighbor process defined above will return some
match even if no actual match exists, handling this case requires a
filter for ``poor'' matches. We implement this process with a filter
on the similarity measure itself: for matches whose similarity
measure $s$ is below a user-supplied threshold $T$, we reject the
match and instead match the section to itself. This provides a set of
valuable information to the analyst, identifying sections of a bill that
came out of negotiated rather than formal amendment processes. 

The use of a similarity threshold raises the question of how to set
that threshold. Too high a threshold will catch all ``unsourced'' bill
sections but throw out accurate matches. Likewise, too low a threshold
will reduce match quality and lead to potentially incorrect
inferences. 

We propose to treat this as a supervised learning problem
in which the optimal $T$ is derived from a set of user-coded data. We
provide tools to automate the hand-matching of a random subset of the
final bill to its potential sources. Based on that hand coding, the
proper threshold value can be chosen to maximize the accuracy of the
matching algorithm. We provide the capability to estimate the optimal
$T$ based on both overall accuracy and the tradeoff of Type 1 and Type
2 errors. 


\section{The technical implementation}
\label{sec:legisl-sect-match}


\subsection{Matching and visual comparison}
\label{sec:match-visu-comp}

Given the model presented in section \ref{sec:how-bill-becomes}, the
analytic problem thus becomes identifying which amendments and
portions of the initial bill become the final bill. We treat this as a
document retrieval problem, wherein we wish to query for a document
$D_{f,i}$ and retrieve its most likely match from $D_i,
A_j$. \footnote{This can, alternatively, be treated as a
subset of the plagiarism problem, wherein we know with probability
approaching one that $D_f$ has plagiarized content from $D_i$ and
$A_j$. As with other versions of plagiarism detection, the problem
then becomes establishing an effective and efficient comparison
strategy.}

We implement a generic comparison strategy in four steps:
\begin{enumerate}
\item Transform each section of $D_f$, $D_i$, and $A_J$ into a
  bag-of-words vector-space representation in which features are
  n-grams of potentially heterogeneous length
\item Construct the pairwise similarity, using some similarity metric,
  between each section of $D_f$ and all sections in $D_i$ and $A_J$.
\item Use nearest-neighbor matching without replacement to establish
  the closest match between each section of $D_f$ and a section in
  $D_i$ and a candidate amendment in $A_J$
\item Pick the closest match of the pair of potential matches $d_i$
  and $a$
\end{enumerate}

Implementation of step (1) occurs via the \texttt{tm} package for R \citep{meyer2008text},
and uses the Weka tokenizer for term
tokenization \citep{hall2009weka}. This infrastructure provides users the ability to
specify whether words should be stemmed; whether punctuation, excess
whitespace or stopwords should be
removed; the minimum and maximum n-gram length used for constructing the vector space
representation of text, and whether the term list should be filtered
on the basis of one of several metrics including term-frequency and
term frequency / inverse document frequency measures. 

The \texttt{leghist} package comes with several alternative distance metrics for use
in step (2). We implement a standard cosine similarity, a
length-weighted cosine similarity, and a length-weighted joint
information distance measure as described by (). In plagiarism applications, length-sensitive
distance metrics have performed well in identifying correct documents
from otherwise quite similar candidate matches. However, as section \ref{} will show,
the matching process here appears to favor the fuzzy matches provided
by the length-insensitive cosine similarity. This may, as we will discuss, be due to the
somewhat arbitrary choice of what constitutes a document section, and
its instability over time. Finally, the function interfaces allow the
user to supply any distance function so long as it returns a
correctly-formatted distance matrix. 

Step (3) uses simple nearest neighbor matching with replacement to
identify the most likely candidate matches from both the initial
document and the set of proposed amendments. Both matches are returned in an
intermediate step to permit users to identify the connection to the
initial bill and to the potential amendments. Finally, under the
assumption that the similarity metric is consistent and well-ordered, we can then in step (4) compare
this pair of potential matches to determine the best potential match
from all candidate matches. 

Based on the pairwise matches between sections in the final bill and
the set of potential sources for the final version of those sections,
we can construct a synthetic bill for comparison purposes. The package
provides tools to construct a side-by-side comparison of the final
bill sections to their candidate matches. If the user supplies a
non-zero threshold value as described in section
\ref{sec:extens-negot-cont}, that
value is used to determine whether the actual match
should be used. If the threshold value is not met, then the final
document section is matched to itself. 

Finally, \texttt{leghist} provides an interface to output the matched
document as a two-column \LaTeX document, which can be directly
compiled into a PDF. Within that document, each paragraph of the final
bill is shown side-by-side with its match. Words differences between
the paragraphs are highlighted as colored text. 

\subsection{Supervised learning of the optimal threshold value}
\label{sec:superv-learn-optim}

Selection of an optimum threshold value will depend on the specifics
of the legislative process behind each bill. While our experiments
suggest that values on the order of 0.3 perform reasonably well, we
recommend that users base threshold selection on the performance of
the matching algorithm in their particular case. To facilitate this process, we provide tools that allow the user to
hand-code a subset of a given bill and derive the optimum threshold
value based on the algorithm's performance against that hand-coded
data. 

\subsection{Amendment content analysis and clustering}
\label{sec:amendm-cont-analys}

Finally, the legislative process rarely amends bills at
random. Rather, specific sections or issue areas within bills will
often prove more contentious than others. The user may wish to
identify these areas on the basis of accepted and rejected amendments,
sections of the original bill that persist to the final bill, and
sections of the final bill for which no match was identified. 

We provide a ready interface between the matched bill and the
\texttt{topicmodels} package for R that enables topic modeling of
these different slices of the matched bill. As described by
\cite{blei2003latent,blei2003latent}, topic modeling assumes a latent
structure within a set of documents. Each document--in this case, a
bill section--is assumed to derive from a mixture of topics. Each
topic, in turn, is represented by distribution over a
set of terms. Latent Dirichlet Allocation provides a Bayesian method
for inferring topics from empirical word distributions, and assigning
documents to those topics on the basis of document-specific
distributions. 



\section{Vignette: the 2009 European Electricity Market Regulation }
\label{sec:vign-2009-europ}

To illustrate each of these tools and their technical underpinnings,
we now provide a vignette based on actual legislation as passed by the
European Parliament. In 2008, the European Commission proposed a new round of reforms to
the regulation of European electricity markets. This reform followed
on the first (1996) and second (2003) internal market directives. Each
aimed to break up the vertically integrated utilities that dominated
national electricity markets, facilitate cross-border market
integration, and support greater consumer choice over energy
suppliers. The reforms also played a facilitating role for the
incorporation of greater amounts of renewable energy into Europe's
electricity supply. 

The original bill in this case was a package of amendments to the 2003
legislation proposed by the European Commission as Commission document
COM 2007 (528). Based on this document, the European Parliament
introduced a first round of amendments through its First Reading. On
the basis of that document, the European Council subsequently
negotiated a Common Position. This replaced the Commission's
legislative proposal as a complete bill. The Parliament subsequently
amended this further, in a Second Reading. The final bill was
adoped as Directive 2009 (72).\footnote{A full procedural record can
  be found at
  \url{http://www.europarl.europa.eu/oeil/popups/ficheprocedure.do?reference=2007/0195(COD)&lg=fr}. Referenced
12 February 2012.} 

This section demonstrates the operation of the \texttt{leghist}
package in analyzing the amendment process for this legislation. We
treat the initial bill as the 2003 legislation which the Commission
proposed to change. Amendments then constitute the Commission proposal
and the amendments contained in the First and Second Reading
Reports. We do not treat the Council Common Position as a set of
amendments, as it was released as a complete bill with no indication
of where the Council had amended it. Instead, we use this as a feature
to demonstrate the ability of the \texttt{leghist} package to reveal
changes not represented in formal amendments. 

We begin by reading in the bills and amendments as plain text. In each
case, the bills had been parsed from original documents available from
the European Legislative Observatory. We note that neither the bills
nor the amendments were
not available in easily machine-readable format. We provide more
detail on how the available electronic records were parsed into
machine-readable plain text in appendix (). 

\begin{lstlisting}[language=R, numbers=none]
ep.first.reading <- 
  read.csv("./2007/txt/ep_first_reading_report.txt",
           header=TRUE,
           stringsAsFactors=FALSE
           )
ep.second.reading <-
  read.csv("./2007/txt/ep_second_reading_report.txt",
           header=TRUE,
           stringsAsFactors=FALSE
           )

initial.bill <- readLines("./2003/txt/directive_2003_54_ec.txt")
commission.proposal <- readLines("./2007/txt/com_2007_528_final.txt")
council.common.position <- readLines("./2007/txt/council_common_position.txt")
final.bill <- readLines("./2007/txt/directive_2009_72_ec.txt")
\end{lstlisting}

We then transform this set of documents into a single document-term
matrix containing a vector space bag-of-words representation of each
section of the bills and proposed amendments. Each row of the matrix represents one section of one document,
while the columns represent unique terms in those documents, and
matrix entries the term frequencies for those terms in each
document. In this case, we do not stem the terms, but do remove
English stopwords, excess whitespace, and punctuation. We also use
bigrams, on the supposition that the added semantic content of unique
2-consecutive-word combinations may give greater resolution.

\begin{lstlisting}[language=R, numbers=none]
doc.list <- CreateAllVectorSpaces(initial.bill,
                                  final.bill,
                                  c(ep.first.reading$text,
                                    ep.second.reading$text,
                                    commission.proposal
                                    ),
                                  ngram.min=1,
                                  ngram.max=3,
                                  stem=FALSE,
                                  rm.stopwords=TRUE,
                                  rm.whitespace=TRUE,
                                  rm.punctuation=TRUE,
                                  filter=NULL,
                                  filter.thres=NULL
                                  )

\end{lstlisting}

The \texttt{doc.list} object takes the form of a list containing four
objects:
\begin{itemize}
\item \texttt{vs.out}, the document-term matrix represented as a
  sparse Matrix object
\item \texttt{idx.final}, an integer vector of row indices
  representing documents in the final bill
\item \texttt{idx.initial}, an integer vector of row indices
  representing documents from the initial bill
\item \texttt{idx.amendments}, an integer vector of row indices
  representing documents from the proposed amendments
\end{itemize} 

With the document-term matrix, we can now map the final bill to its
potential matches in the initial bill and amendments. The function
\texttt{MapBills} takes as input the \texttt{doc.list} object and a
specification for the distance function. Here, we use
\texttt{cosine.mat}, a version of the \texttt{cosine} distance
optimized for this use case by using matrix algebra to compute all
distances simultaneously. However, any function that returns a
distance matrix of the $N_{final} \times N_{compare}$, where the rows
are the sections of the final bill and columns the sections of the
potential matches, may be used. 

\begin{lstlisting}[language=R, numbers=none]
map.bills.cos <- MapBills(doc.list,
                          distance.fun="cosine.mat"
                          )
\end{lstlisting}

\texttt{MapBills} returns an R data frame with five columns: the
section index of the final bill, and the section index and distance
measure of both the original bill match and the amendment match. We
can see a sample of that data frame here:

\begin{lstlisting}
  
\end{lstlisting}

Before we identify the most likely set of matches based on these
distance metrics, we want to pick the optimum threshold value as
discussed on section \ref{sec:extens-negot-cont}. To do so, we will
use a hand-coded set of best matchs. The \texttt{leghist} package
provides three tools for doing so. First, the user may encode the best
match from the set of candidates using the same structure used by
\texttt{MapBills}. The \texttt{run.encoder} routine allows the user to
encode a randomly-sampled portion of the final bill. It presents the
user with a set of candidate matches, based on likely matches from
both the initial bill and amendments. The user provides the index of
the best match, or \texttt{NA} if no valid match is
available.\footnote{We note that users who do not want to use the
  automated matching tools may still use the encoding tools to
  hand-match amendments. The tools reduce the set of possible matches
  to a set of likely matches based on the same vector space language
  representation used by the automated match algorithms, thereby
  reducing the search space for hand-coded matches.} Figure
\ref{fig:encoder-interface} provides a sample of the 
\texttt{run.encoder} interface.

Based on the encoded sample, we may identify the optimum threshold
value based on the empirical accuracy of the matching algorithm. To do
so, we provide the \texttt{learn.threshold} interface. This takes as
one of its arguments a sequence of potential threshold values. It then
computes the accuracy of synthetic bill based on the output from \texttt{MapBills},
for each threshold value, and returns the optimum threshold value from
the sequence. The optimum
threshold value may be identified in two ways based on the
\texttt{type} argument. With \texttt{type=overall},
the aggregate accuracy of the matched pairs is estimated, and the
threshold value corresponding to the highest accuracy returned. With
\texttt{type=tradeoff}, the Type 1/2 error tradeoff is estimated and
the threshold value at the intersection of the type 1 and type 2 error
curves is returned. We recommend that the threshold sequence be
specified at a fairly granular level, such as \texttt{seq(0, 0.5, 0.005)}.

Figure \ref{fig:learn-threshold-output} shows the
output of both versions of this function as trained on a 30\% sample
of the bill in question. We can see that the optimum choice of
threshold value is about 0.3, such that any matches with simliarity
values $s < 0.3$ should be treated as ``no match''.

\begin{figure}[ht]
  \centering
  
  \caption{Accuracy curves as returned from the \texttt{learn.threshold} function. Notice that the optimum threshold value is closer to the accuracy curve than the type 1/2 tradeoff curve intersection, owing to the relative flatness of the tradeoff curves.}
  \label{fig:learn-threshold-output}
\end{figure}

With an optimum threshold in mind, we can now build the most likely
composite on the basis of the matched
bill. \texttt{GetLikelyComposite} provides functionality for doing
so. As we can see, this takes a series of arguments: the output from
MapBills, the plain text used to create the original \texttt{doc.list}
object, and a series of labels that indicate from where the
amendments came (in this case, the Parliamentary committees from the
first reading, the Parliamentary second reading, and the European
Commission). The argument \texttt{dist.threshold} represents the decision threshold $T$ described in section
\ref{sec:extens-negot-cont}. If the program is unable to find a ``best match'' whose
distance to the target section of the final document exceeds that
value, it will insert the same section of the "final" bill as the best
match. This can be used to identify sections of the bill with
uncertain origins, which perhaps arose outside of the formal amendment
process. 


\begin{lstlisting}
  composite.bill.cos <- 
    GetLikelyComposite(map.bills.cos,
                       initial.bill,
                       final.bill,
                       c(ep.first.reading$text,
                       ep.second.reading$text,
                       commission.proposal
                       ),
                       c(ep.first.reading$committee,
                       rep("Parliament 2nd Reading", 
                            nrow(ep.second.reading)
                          ),
                       rep("Commission 2007",
                       length(commission.proposal)
                       )
                       ),
                       filter="max",
                       dist.threshold=0.2
                       )
                       
\end{lstlisting}

Finally, we may wish to write out a side-by-side comparison of the
actual final bill with the ``best match'' synthetic version returned
by \texttt{GetLikelyComposite}. The 
\texttt{WriteSideBySide} function provides an interface for building a
2-columna \LaTeX document with that comparison. Margin notes indicate the source of the
matched paragraph, its index number, and the distance or similarity
measure for its match to the target paragraph in the final
bill. Sample PDF output is shown in figure \ref{fig:pdf-output}.

\begin{lstlisting}[language=R]
write.side.by.side.cos <- 
   WriteSideBySide(composite.bill.cos,
                   final.bill,
                   cavs.out=doc.list,
                   file.out="ep_2007_intl_mkt_sbs_cos.tex",
                   dir.out="./2007/tex/",
                   pdflatex=TRUE
                   )
\end{lstlisting}

\begin{figure}[ht]
  \centering
  \includegraphics[width=0.75\textwidth]{sample_pdf_output}
  \label{fig:pdf-output}
  \caption{Sample PDF output from the \texttt{WriteSideBySide} function, for the 2009 European energy market reform legislation.}
\end{figure}

\section{Accuracy trials}
\label{sec:accuracy-trials}

We assess the accuracy of the matching algorithm against two different
standards: a human-matched document and the legislative record of
adopted amendments. The human-matched document makes use of the
\texttt{encoder} tools provided in \texttt{leghist}. These tools take
as input the same set of documents provided to the matching algorithm,
generate a set of candidate matches for each section in the final
bill, and provide a user interface to indicate which of the candidate
matches is in fact the best match. A sample of the user interface,
running in the R console, is show in figure \ref{fig:encoder-interface}.

\begin{figure}[ht]
  \centering
  \includegraphics[width=\textwidth]{encoder}
  \caption{Sample of the \texttt{Encoder} interface, running in the R console. }
  \label{fig:encoder-interface}
\end{figure}

For the legislative record, we used the record of European
Parliamentary debate for the 2009 energy market reform. The EU process
provides a particularly interesting test of the tools given its
somewhat convoluted process. After the Commission proposes legislation
to the Parliament, one or more committees can propose amendments,
which are numbered $1:N$ for each committee. Some subset of those
amendments are adopted, but the final adoption record does not
explicitly number the amendments or provide a mapping from the new
legislative document to the amendments. The Commission then indicates
which amendments it considers acceptable. The revised legislation and
the Commission's comment document are then taken up by the Council,
which can also amend the document. The Council, however, provides no
record of its internal proceedings and does not indicate, in the
version of the legislation it produces, which sections it
changed. Finally, the Parliament can conduct a second reading, where
it can again propose amendments. Those amendments may represent
agreed-upon changes to the bill arrived at through negotiation with
the Council, or new amendments. The final bill, ultimately, provides
no indication of which, if any, of these amendments were
adopted. Furthermore, subtle changes in language may occur to any
section of the bill, or to accepted amendments, during the legislative
process. 

For the first
Parliamentary reading, the Commission provided detailed responses on
each adopted Parliamentary amendment, which shaped which amendments
were accepted as part of the Co-Decision process between the
Commission, the Council, and the Parliament. We use that record to
identify actual adopted amendments from the first reading. For the
second reading, however, we do not have any such record. 


\section{Content modeling}
\label{sec:content-modeling}

Finally, given the number of potential changes, we may wish to
summarize the content of those changes. This may not, of course,
capture subtle changes to wording that make large differences to the
impact of legislation. But it can help identify issue areas or policy
domains in which the legislative process made or failed to make
broader changes. It can also help identify areas of contention within
the broader domain of legislation. 

The \texttt{leghist} package provides the \texttt{ModelDocSet}
function as an interface to topic model capabilities in the
\texttt{topicmodels} package \citep{grun2011topicmodels}. These, in
turn, provide R interfaces to the Latent Dirichelet Allocation methods
implemented by \cite{blei2003latent} and
\cite{blei2006correlated}. These methods infer a set of topics from
the term distributions over a set of documents, on the assumption that
topics are differentiated by different distributions over a set of
terms. \texttt{ModelDocSet} provides the ability to feed one of four
subsets of documents to these models: the included or rejected
amendments; and the surviving or rejected portions of the original
bill. 

\begin{lstlisting}[language=R, numbers=none]
model.incl.amend <- ModelDocSet(doc.list,
                                composite.bill.cos,
                                type="incl.amend",
                                k=5,
                                control=list(seed=2342)
                                )
model.rej.amend <- ModelDocSet(doc.list,
                               composite.bill.cos,
                               type="rej.amend",
                               k=5,
                               control=list(seed=2342)
                               )
\end{lstlisting}



\bibliography{/Users/markhuberty/Documents/Research/Papers/leghist/bib/leghist}
\bibliographystyle{apalike}
\end{document}

%% Want to be very specific that this is only pointing to the Morgan
%% report, which deals w/ internal market rules, and not to the other
%% reports, see here:

%% WE deal w/ this one
% Recommendation for second reading: on the Council common position for adopting a directive of the European Parliament and of the Council concerning common rules for the internal market in electricity and repealing Directive 2003/54/EC [14539/2/2008 - C6-0024/2009 - 2007/0195(COD)] - Committee on Industry, Research and Energy. Rapporteur: Eluned Morgan (A6-0216/2009)

%% Not these, which are separate...
% Recommendation for second reading: on the Council common position for adopting a regulation of the European Parliament and of the Council establishing an Agency for the Cooperation of Energy Regulators [14541/1/2008 - C6-0020/2009 - 2007/0197(COD)] - Committee on Industry, Research and Energy. Rapporteur: Giles Chichester (A6-0235/2009)

% Recommendation for second reading: on the Council common position for adopting a regulation of the European Parliament and of the Council on conditions for access to the network for cross-border exchanges in electricity and repealing Regulation (EC) No 1228/2003 [14546/2/2008 - C6-0022/2009 - 2007/0198(COD)] - Committee on Industry, Research and Energy. Rapporteur: Alejo Vidal-Quadras (A6-0213/2009)

% Recommendation for second reading: on the Council common position for adopting a directive of the European Parliament and of the Council concerning common rules for the internal market in natural gas and repealing Directive 2003/55/EC [14540/2/2008 - C6-0021/2009 - 2007/0196(COD)] - Committee on Industry, Research and Energy. Rapporteur: Antonio Mussa (A6-0238/2009)

% Recommendation for second reading: on the Council common position for adopting a regulation of the European Parliament and of the Council on conditions for access to the natural gas transmission networks and repealing Regulation (EC) No 1775/2005 [14548/2/2008 - C6-0023/2009 - 2007/0199(COD)] - Committee on Industry, Research and Energy. Rapporteur: Atanas Paparizov (A6-0237/2009)