* Design notes
1. Conceptual framework:
   For a piece of legislation L that goes through V versions L_v, and is
   subject to A proposed amendments, we would like to know how that
   legislation changes across the legislative cycle, which amendments
   were adopted, and whether they represent changes to substantive or
   administrative aspects of the law (i.e., what the law is intended
   to accomplish, versus how it should accomplish it). 
2. Automating this process would ideally return, for any two versions
   L_1 and L_2, several pieces of information:
   + A mapping of the sections of L_1 to their new location in L_2
   + A probabilistic indicator of the confidence of the map
   + A summary of the new content added
   + A summary of the content taken away
   + An indication of whether the changes were substantive or
     administrative
3. This implies several procedural steps:
   + Convert legislation to plain text and strip out any unnecessary information
   + Separate plain text into a set of individual "documents" that are
     at a sufficient level of granularity (paragraphs, perhaps?)
   + Do this for all versions to be compared
   + Construct a pairwise distance measure between each "document" in
     version 1 and each "document" in version 2
   + Select the "most likely" match and return it with a probability score
   + If amendments are provided, compare "not found" or low-prob match
     sections with amendments and return most likely amendment for
     that section
   + Return the original document, the matched sections of the new
     document
   + Run a topic model on stuff excluded from V1 in V2, new stuff in
     V2, amendments rejected, amendments accepted. 
   + Classify changes as substantive or administrative--perhaps based
     on a curated word list that would do well for administrative
     (i.e. "report" "enforce" "monitor" "transpose", etc)
   + For converting EU .docs to plain text, best way appears to be:
     save as .htm, then run lynx -dump <file> > output.txt. That
     strips the cruft out and leaves documents
     well-formatted. Produces (if a .doc is all that's available)
     about the same result as lynx -dump 'url.to.html.version' >
     output.txt. 
4. Some logic issues:
   + General structure:
     - first map bill_1:bill_2
     - then if(blank) map amend:blank
     - for chunk in bill2, take better match of bill1 and amend
     - Unless amend=delete? unclear how to handle this
   + Implementation
     - Need to construct the pairwise distance for all paras in bill1:2
     - And for amendments 1:2. These should produce lists of length bill2
     - Then take pairwise mins and return source (b, a) and index
     - Then retrieve correct chunk from amend or bill1 record w/ index
   + Should pass bills and amendments as lists; each element in the
     list is the string chunk vector. length(amendments) <
     (length(bills) - 1) so that you don't have more instances of
     amendments than you do of amending sessions. But amendments could
     be NULL. 
     - Then just loop across the bill structure--that way the function
       does both 2-version comparision and n-version comparison. 
     - To do this efficiently, pre-compute the corpuses so that only
       happens 1x for both the bills and amendments

* Unknowns:
  1. Correct distance measure
  2. Probabilistic measure--what's the probability derived from?
  3. What's the input to the topic model--the amendments themselves,
     the portions that were included / excluded, etc?
  4. How should classification be presented?

* TODO See Lee et al, "An Empirical Evaluation of Models of Text Document Similarity" for weighting /distance

* TODO LSA models for document similarity in R, the LSA package at CRAN: http://cran.r-project.org/web/packages/lsa/lsa.pdf



